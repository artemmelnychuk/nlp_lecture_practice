{"cells":[{"cell_type":"markdown","metadata":{"id":"ZBqubaUqATYg"},"source":["# 0 Preparations\n","First, install the packages needed in this notebook:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"JRvxnNpYimUv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers[torch] in ./.venv/lib/python3.10/site-packages (4.41.2)\n","Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (2.20.0)\n","Requirement already satisfied: evaluate in ./.venv/lib/python3.10/site-packages (0.4.2)\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting sacrebleu\n","  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting spacy\n","  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\n","Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n","Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (24.1)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (3.15.1)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.23.4)\n","Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.31.0)\n","Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.3.1)\n","Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets) (2024.5.0)\n","Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from bert_score) (3.9.0)\n","Collecting portalocker\n","  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n","Collecting tabulate>=0.8.9\n","  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n","Collecting lxml\n","  Downloading lxml-5.2.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting colorama\n","  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting typer<1.0.0,>=0.3.0\n","  Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n","Collecting wasabi<1.2.0,>=0.9.1\n","  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n","Collecting preshed<3.1.0,>=3.0.2\n","  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n","Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n","  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n","  Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n","  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy) (3.1.4)\n","Collecting spacy-legacy<3.1.0,>=3.0.11\n","  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n","Collecting murmurhash<1.1.0,>=0.28.0\n","  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n","Collecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n","Collecting cymem<2.1.0,>=2.0.2\n","  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 KB\u001b[0m \u001b[31m214.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n","  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting absl-py\n","  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nltk\n","  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n","Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n","Collecting language-data>=1.2\n","  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Collecting annotated-types>=0.4.0\n","  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n","Collecting pydantic-core==2.20.1\n","  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.6.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.2)\n","Collecting blis<0.8.0,>=0.7.8\n","  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hCollecting numpy>=1.17\n","  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Collecting confection<1.0.0,>=0.0.1\n","  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n","Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (2.20.5)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (2.3.1)\n","Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n","Collecting shellingham>=1.3.0\n","  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Collecting click>=8.0.0\n","  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Collecting rich>=10.11.0\n","  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n","Collecting cloudpathlib<1.0.0,>=0.7.0\n","  Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting smart-open<8.0.0,>=5.2.1\n","  Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (3.1.2)\n","Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (10.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.1)\n","Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n","Collecting marisa-trie>=0.7.7\n","  Downloading marisa_trie-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Collecting markdown-it-py>=2.2.0\n","  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","Collecting wrapt\n","  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Collecting mdurl~=0.1\n","  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Using legacy 'setup.py install' for rouge_score, since package 'wheel' is not installed.\n","Installing collected packages: cymem, wrapt, wasabi, tabulate, spacy-loggers, spacy-legacy, shellingham, pydantic-core, portalocker, numpy, murmurhash, mdurl, marisa-trie, lxml, colorama, cloudpathlib, click, catalogue, annotated-types, absl-py, srsly, smart-open, sacrebleu, pydantic, preshed, nltk, markdown-it-py, language-data, blis, rouge_score, rich, langcodes, confection, typer, thinc, weasel, bert_score, spacy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.0\n","    Uninstalling numpy-2.0.0:\n","      Successfully uninstalled numpy-2.0.0\n","  Running setup.py install for rouge_score ... \u001b[?25ldone\n","\u001b[?25hSuccessfully installed absl-py-2.1.0 annotated-types-0.7.0 bert_score-0.3.13 blis-0.7.11 catalogue-2.0.10 click-8.1.7 cloudpathlib-0.18.1 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 lxml-5.2.2 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 nltk-3.8.1 numpy-1.26.4 portalocker-2.10.0 preshed-3.0.9 pydantic-2.8.2 pydantic-core-2.20.1 rich-13.7.1 rouge_score-0.1.2 sacrebleu-2.4.2 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 tabulate-0.9.0 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n","Collecting git+https://github.com/google-research/bleurt.git\n","  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-3f6__zxv\n","  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-3f6__zxv\n","  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from BLEURT==0.0.2) (1.26.4)\n","Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from BLEURT==0.0.2) (2.2.2)\n","Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from BLEURT==0.0.2) (1.13.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting tensorflow\n","  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting tf-slim>=1.1\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in ./.venv/lib/python3.10/site-packages (from tf-slim>=1.1->BLEURT==0.0.2) (2.1.0)\n","Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->BLEURT==0.0.2) (2024.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\n","Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->BLEURT==0.0.2) (2024.1)\n","Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (24.1)\n","Collecting ml-dtypes~=0.3.1\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting google-pasta>=0.1.1\n","  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","Collecting h5py>=3.10.0\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\n","Collecting termcolor>=1.1.0\n","  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n","  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","Collecting tensorboard<2.17,>=2.16\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting flatbuffers>=23.5.26\n","  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n","Collecting grpcio<2.0,>=1.24.3\n","  Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (59.6.0)\n","Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n","  Downloading gast-0.6.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0\n","  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n","Collecting astunparse>=1.6.0\n","  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n","Collecting libclang>=13.0.0\n","  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n","Collecting wheel<1.0,>=0.23.0\n","  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n","Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow->BLEURT==0.0.2) (13.7.1)\n","Collecting optree\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 KB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting namex\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2024.6.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.7)\n","Collecting werkzeug>=1.0.1\n","  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 KB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow->BLEURT==0.0.2) (2.1.5)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2) (2.18.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n","Using legacy 'setup.py install' for BLEURT, since package 'wheel' is not installed.\n","Using legacy 'setup.py install' for gast, since package 'wheel' is not installed.\n","Installing collected packages: sentencepiece, namex, libclang, flatbuffers, wheel, werkzeug, tf-slim, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, ml-dtypes, markdown, h5py, grpcio, google-pasta, gast, tensorboard, astunparse, keras, tensorflow, BLEURT\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.0\n","    Uninstalling ml-dtypes-0.4.0:\n","      Successfully uninstalled ml-dtypes-0.4.0\n","  Running setup.py install for gast ... \u001b[?25ldone\n","\u001b[?25h  Running setup.py install for BLEURT ... \u001b[?25ldone\n","\u001b[?25hSuccessfully installed BLEURT-0.0.2 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 protobuf-4.25.3 sentencepiece-0.2.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 tf-slim-1.1.0 werkzeug-3.0.3 wheel-0.43.0\n"]}],"source":["! pip install transformers[torch] datasets evaluate bert_score sacrebleu spacy rouge_score\n","! pip install git+https://github.com/google-research/bleurt.git"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1Pj6pncLzoIN"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/artem/programming/nlp_lecture_practice/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in ./.venv/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# Downlaod spacy model\n","! python -m spacy download en_core_web_sm"]},{"cell_type":"markdown","metadata":{"id":"6wEE-KgBF37n"},"source":["# 1 Seq2seq evaluation metrics"]},{"cell_type":"markdown","metadata":{"id":"gbb0ItNfF_X6"},"source":["### 1.1 You are given a candidate and a reference translation and the score of a metric. What type of metrics was used? Can you suggest better metric? Justify your answer!\n","\n","```\n","Reference: \"My cat loves to watch the birds outside the window.\"\n","Candidate: \"My cat hates to watch the birds outside the window.\"\n","-> score: 0.99\n","```"]},{"cell_type":"markdown","metadata":{"id":"sudwxCRnpVQ0"},"source":["### 1.2 You want to train a machine translation system but you only have a few thousand aligned sentences. Are there metrics that are especially suited for this low-resource scenario? Why?\n"]},{"cell_type":"markdown","metadata":{"id":"ALYwRZTmpX2r"},"source":["### 1.3 Your friend tells you this: \"I cannot use a learned metric for my task because my data is from a very special domain and there will be a domain mismatch.\" - Is she right? Does she miss something?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RLAs3SyaZ-En"},"source":["## 1.4 Recreate the scores from the lecture slides with Huggingface evaluate"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"aM5KQ-c8hZtY"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-04 11:44:46.762855: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-07-04 11:44:46.778412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-04 11:44:46.800730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-04 11:44:46.800767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-04 11:44:46.814338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-04 11:44:47.762087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-07-04 11:45:22.931508: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"]}],"source":["%%capture\n","from evaluate import load # use the Huggingface evaluate implementations\n","bertscore = load(\"bertscore\")\n","bleu = load(\"sacrebleu\")\n","bleurt = load(\"bleurt\", module_type=\"metric\", checkpoint=\"Elron/bleurt-base-128\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HwVTuZoUqS4E"},"outputs":[{"name":"stdout","output_type":"stream","text":["59.460355750136046\n","41.154215810165745\n","64.75445426291287\n"]}],"source":["print(bleu.compute(predictions=[\"My weekend was bad\"], references=[\"My weekend was superb\"])['score'])\n","print(bleu.compute(predictions=[\"At the weekend, we ate my grandma's house.\"], references=[\"At the weekend, we visited my grandma's house and ate cake.\"])['score'])\n","print(bleu.compute(predictions=[\"At the weekend, we visited my grandma's house. And we ate cake.\"], references=[\"At the weekend, we visited my grandma's house and ate cake.\"])['score'])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7roTRTZQjR_H"},"outputs":[],"source":["# This function makes comparing different scores for a given reference-candidate pair more handy\n","def evaluate_and_compare_scores(reference: str, candidate: str, language: str='en') -> None:\n","    print(\"Reference: \", reference)\n","    print(\"Candidate: \", candidate)\n","\n","    score_bleu = bleu.compute(predictions=[candidate], references=[reference], smooth_method='none')['score']\n","    print(f\"BLEU: {score_bleu}\")\n","    score_bertscore = bertscore.compute(predictions=[candidate], references=[reference], lang=language)['f1']\n","    print(f\"BERTscore: {score_bertscore}\")\n","    score_bleurt = bleurt.compute(predictions=[candidate], references=[reference])['scores']\n","    print(f\"BlEURT: {score_bleurt}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3MQR_khFkNCG"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reference:  I feel good.\n","Candidate:  I feel amazing.\n","BLEU: 0.0\n","BERTscore: [0.9791091084480286]\n","BlEURT: [0.761749267578125]\n","***\n","Reference:  I feel good.\n","Candidate:  good I feel.\n","BLEU: 0.0\n","BERTscore: [0.9251930713653564]\n","BlEURT: [0.48538005352020264]\n","***\n","Reference:  I feel good.\n","Candidate:  I feel good, at least today .\n","BLEU: 0.0\n","BERTscore: [0.9391999840736389]\n","BlEURT: [0.14559583365917206]\n","***\n","Reference:  Dieses Haus ist in einer großen Stadt.\n","Candidate:  Das Haus in einer großen Stadt ist.\n","BLEU: 39.76353643835254\n","BERTscore: [0.9289785027503967]\n","BlEURT: [0.41286009550094604]\n"]}],"source":["####################################################################\n","# TODO come up with own examples and try to fool the scores\n","# Can you make further observations?\n","####################################################################\n","ref = \"I feel good.\"\n","cands = [\"I feel amazing.\", \"good I feel.\", \"I feel good, at least today .\"]\n","####################################################################\n","for cand in cands:\n","    evaluate_and_compare_scores(ref, cand)\n","    print('***')\n","\n","ref_de = \"Dieses Haus ist in einer großen Stadt.\"\n","cand_de = \"Das Haus in einer großen Stadt ist.\"\n","evaluate_and_compare_scores(ref_de, cand_de, language='de')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sozmL3LcCDuL"},"outputs":[],"source":["####################################################################\n","# TODO Look at the Huggingface metrics page (https://huggingface.co/metrics)\n","# Select two additional metrics and test them on our sample sentences\n","# Note!: you may have to install additional packages to use these metrics!\n","####################################################################\n","metric1 = None\n","metric2 = None\n","####################################################################\n","\n","for cand in cands:\n","  print(\"Reference: \", ref)\n","  print(\"Candidate: \", cand)\n","  print(f\"{metric1.name}: \", metric1.compute(predictions=[cand], references=[ref]))\n","  print(f\"{metric2.name}: \", metric2.compute(predictions=[cand], references=[ref]))"]},{"cell_type":"markdown","metadata":{"id":"U0CRI_s3Ycqp"},"source":["## 1.5 Explain the predicted scores"]},{"cell_type":"markdown","metadata":{"id":"qu8VV_JaoCLH"},"source":["Instead of using the Huggingface evaluate library, you can also load the scoring models with the transformers library. With this, you can use any explainability framework that can interact with Huggingface to explain your score."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wwygQL-jtEw5"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"iCwn2Fg4tJqM"},"outputs":[],"source":["#%%capture\n","import torch\n","model_name = \"Elron/bleurt-base-128\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","model.eval()\n","\n","def predict_bleurt_score(reference:str, candidate:str) -> None:\n","    print(\"Reference: \", reference)\n","    print(\"Candidate: \", candidate)\n","    ####################################################################\n","    # TODO Tokenize the reference and candidate and feed the tokenizer\n","    # output into the model. Print the score prediction.\n","    ####################################################################\n","    # Tokenize the reference and candidate\n","    inputs = tokenizer(reference, candidate, return_tensors=\"pt\", truncation=True, padding=True)\n","    \n","    # Feed the tokenized inputs into the model\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    \n","    # The model outputs a dictionary, we need to extract the score\n","    score = outputs.logits.squeeze().item()\n","    \n","    # Print the score prediction\n","    print(\"BLEURT Score: \", score)\n","    ####################################################################"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"UOT3isefLAv9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reference:  At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids.\n","Candidate:  At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. It was really delicious! Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids.\n","BLEURT Score:  0.07780024409294128\n","***\n","Reference:  At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids.\n","Candidate:  At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids. It was really delicious!\n","BLEURT Score:  0.6657406687736511\n"]}],"source":["ref = (\"At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. \"\n","  \"Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids.\")\n","cand = (\"At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. It was really delicious! \"\n","  \"Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids.\")\n","cand2 = (\"At the weekend, we visited my grandma's house and ate cake. She has baked a chocolate cake especially for me as it is my favourite cake. \"\n","  \"Afterwards, we went for a long walk across the fields. The weather was superb and we saw a lot of birds, squirrels and even some wild rabbids. It was really delicious!\")\n","predict_bleurt_score(ref, cand)\n","print('***')\n","predict_bleurt_score(ref, cand2)"]},{"cell_type":"markdown","metadata":{"id":"G_Yts9WBQArw"},"source":["### Both candidates hallucinate \"It was really delicious!\". However, the second candidate does not seem to get punished for it. Can you think of an explanation why?\n"]},{"cell_type":"markdown","metadata":{"id":"TZ5AZdKOgKbr"},"source":["# 2 Faithfulness"]},{"cell_type":"markdown","metadata":{"id":"dHnDLYuyx2PJ"},"source":["In this section, we fine-tune a question generation system to create a question-answering based hallucination detection system.\n","\n","The steps for such a system are:\n","\n","\n","1.   Answer span extraction\n","2.   Question generation\n","3.   Question answering\n","4.   Answer comparison\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"T1d11cUvmMzY"},"outputs":[],"source":["####################################################################\n","# TODO think of additional candidates that you want to evaluate\n","####################################################################\n","source = \"John became an older brother because Mary gave birth to a girl.\"\n","candidates = [ \"\"\n","]\n","####################################################################"]},{"cell_type":"markdown","metadata":{"id":"0c3Wx3ThyxZr"},"source":["## 2.1 Answer span extraction"]},{"cell_type":"markdown","metadata":{"id":"PVqfalm8y0QH"},"source":["For simplicity, we will only focus on noun answers.\n","\n","Parse the candidates with spacy and extract all nouns."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"r8UeMc_b0y1O"},"outputs":[{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Parse the first candidate and print its annotations.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(token\u001b[38;5;241m.\u001b[39mtext, token\u001b[38;5;241m.\u001b[39mdep_, token\u001b[38;5;241m.\u001b[39mpos_, token\u001b[38;5;241m.\u001b[39mmorph)\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Parse the first candidate and print its annotations.\n","doc = nlp(candidates[0])\n","for token in doc:\n","  print(token.text, token.dep_, token.pos_, token.morph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXkF5UwizC9M"},"outputs":[],"source":["# Extract all nouns from the candidates\n","\n","answers = {candidate: [] for candidate in candidates}\n","for candidate in candidates:\n","  ####################################################################\n","  # TODO parse the candidate with spacy and append all noun tokens to\n","  # the answers of that candidate\n","  ####################################################################\n","\n","  ####################################################################\n","answers"]},{"cell_type":"markdown","metadata":{"id":"I1TDdsjM2RoG"},"source":["## 2.2.1 Train a question generation system"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmmdZtTs2V90"},"outputs":[],"source":["# Load the SQuAD dataset\n","from datasets import load_dataset\n","\n","squad = load_dataset(\"squad\", split=\"train[:5000]\")\n","squad = squad.train_test_split(test_size=0.2)\n","squad[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CTnhCmt2x5v"},"outputs":[],"source":["# Load the model's tokenizer\n","from transformers import AutoTokenizer\n","\n","model_name = \"google/flan-t5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","tokenizer_args = {\n","    #\"padding\": \"max_length\",\n","    #\"return_tensors\": \"pt\",\n","    \"truncation\": True\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZ8NwntL3miv"},"outputs":[],"source":["def prompt_pattern(answer, context):\n","  ####################################################################\n","  # TODO Design a prompt pattern for the question generation\n","  ####################################################################\n","  prompt = f\"answer: {answer} context: {context}\"\n","  ####################################################################\n","  return prompt\n","\n","def preprocess(samples):\n","  ####################################################################\n","  # TODO Write a preprocessing function:\n","  # 1. Combine the answers and the contexts in a prompt\n","  # 2. Tokenize the inputs\n","  # 3. Tokenize the questions\n","  ####################################################################\n","  inputs = None\n","  ####################################################################\n","  return inputs\n","\n","tokenized_squad = squad.map(preprocess, batched=True, remove_columns=squad[\"train\"].column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDDmgEFj6uoc"},"outputs":[],"source":["# Load the model\n","from transformers import T5ForConditionalGeneration\n","\n","model = T5ForConditionalGeneration.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEXYgPbY6z6L"},"outputs":[],"source":["# Train the model\n","from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"my_awesome_qg_model\",\n","    ####################################################################\n","    # Set the hyperparameters for training\n","    ####################################################################\n","\n","    ####################################################################\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer)\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_squad[\"train\"],\n","    eval_dataset=tokenized_squad[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awR0M16NDFjG"},"outputs":[],"source":["model.save_pretrained(\"my_awesome_qg_model\")"]},{"cell_type":"markdown","metadata":{"id":"5fD1NuXDO8Sv"},"source":["## 2.2.2 Generate questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jh_7JaLMC61s"},"outputs":[],"source":["from transformers import pipeline\n","\n","question_generator = pipeline(\"text2text-generation\", model=\"/content/my_awesome_qg_model\", tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVuK8ol-C7I7"},"outputs":[],"source":["questions = {candidate: [] for candidate in candidates}\n","for candidate in candidates:\n","  ####################################################################\n","  # TODO Use the trained model to extract questions for our samples\n","  ####################################################################\n","\n","  ####################################################################\n","questions"]},{"cell_type":"markdown","metadata":{"id":"lAh2F_wNPHLe"},"source":["## 2.3 Question answering"]},{"cell_type":"markdown","metadata":{"id":"8gj0kIKnU72_"},"source":["Open the [HuggingFace model hub](https://huggingface.co/models) and search for a suitable question answering model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWLRbsOOPXFD"},"outputs":[],"source":["from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n","\n","####################################################################\n","# TODO Load the model and write a function to call the model and\n","# retrieve the answer based on the context\n","####################################################################\n","model_name = \"\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelWithLMHead.from_pretrained(model_name)\n","\n","def question_answering(question, context):\n","  pass\n","####################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGnF-0TppuIA"},"outputs":[],"source":["for candidate in candidates:\n","  print(\"****\", candidate)\n","  for answer, question in zip(answers[candidate], questions[candidate]):\n","    print(\"\\t\", question)\n","    print(\"\\t\\t Original answer:\", answer)\n","    print(\"\\t\\t Answer candidate:\", question_answering(question, candidate))\n","    print(\"\\t\\t Answer source:\", question_answering(question, source))"]},{"cell_type":"markdown","metadata":{"id":"AQh5OQaHPsXa"},"source":["### **Discussion**\n","*  Did you find any hallucinations?\n","*  What kind of hallucinations cannot be detected with such a system?\n","*  What system could you use for these hallucinations?\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
